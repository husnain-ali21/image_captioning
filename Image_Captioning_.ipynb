{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fq6kG-TXCrgU8ONUXxrmhqVrjZuiDRhV",
      "authorship_tag": "ABX9TyPEUlAtzWTb79IySUiUuoC+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojo1sDNILR2B"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import h5py \n",
        "import json \n",
        "import torch\n",
        "from imageio import  imread #or import matplotlib.image as mpimg and mpimg.imread\n",
        "from PIL import Image #resized_img = Image.fromarray(orj_img).resize(size=(new_h, new_w))\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed ,choice ,sample\n",
        "from torch.utils.data  import Dataset,DataLoader\n",
        "import torchvision\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C15PcLvlnkfX"
      },
      "source": [
        "def create_input_files( karpathy_json_path ,image_folder, captions_per_image \n",
        "                       ,min_word_freq , output_folder ,max_len = 50): \n",
        "  \"\"\"\n",
        "  Funtion that prepares data for training,validation and testing and\n",
        "  saves in HDF5 and json format as it is an efficient way of loading data using pyTorch.\n",
        "\n",
        "  Parameters : \n",
        "  -karpathy_json_path : path for the splits and captions\n",
        "  -image_folder : folder with downloaded images \n",
        "  -captions_per_image : no of captions to sample per image\n",
        "  -min_word_freq : a threshold for words to be declared as <UNK>s\n",
        "  -output_folder : folder to save prepared h5py files \n",
        "  -max_len : maximum sequence length \n",
        "  \"\"\"\n",
        "\n",
        "  with open(karpathy_json_path , 'r') as f :\n",
        "    data = json.load(f)\n",
        "  \n",
        "  training_images_paths ,training_images_captions = [] ,[]\n",
        "  val_images_paths , val_images_captions = [],[]\n",
        "  test_images_paths , test_images_captions = [],[]\n",
        "  word_frequencies = Counter()\n",
        "\n",
        "  for image in data[\"images\"]:\n",
        "    captions = [] #a list that stores all appropriate captions of an image \n",
        "    for each_caption in image[\"sentences\"]:\n",
        "      #update word_frequencies\n",
        "      word_frequencies.update(each_caption[\"tokens\"])\n",
        "      if len(each_caption[\"tokens\"]) <= max_len:#discard too long captions\n",
        "        captions.append(each_caption[\"tokens\"])\n",
        "\n",
        "    if len(captions) ==  0 :\n",
        "        continue #we skip the image as we don't need to do anything further\n",
        "      \n",
        "    path_to_an_image = os.path.join(image_folder ,image[\"filename\"])\n",
        "\n",
        "    if image['split'] in {'train' , 'restval'} :\n",
        "      training_images_paths.append(path_to_an_image)\n",
        "      training_images_captions.append(captions)\n",
        "    elif image['split'] == 'val' :\n",
        "      val_images_paths.append(path_to_an_image)\n",
        "      val_images_captions.append(captions)\n",
        "    else :\n",
        "      test_images_paths.append(path_to_an_image)\n",
        "      test_images_captions.append(captions)\n",
        "    \n",
        "  assert len(training_images_paths) == len(training_images_captions) \n",
        "  assert len(val_images_paths) == len(val_images_captions)\n",
        "  assert len(test_images_paths) == len(test_images_captions)\n",
        "\n",
        "  print(f'training dataset size :{len(training_images_paths)}')\n",
        "  print(f'validation dataset size :{len(val_images_paths)}')\n",
        "  print(f'test dataset size :{len(test_images_paths)}')\n",
        "\n",
        "  #Vocabulary\n",
        "  words = ['<PAD>' ,'<UNK>' , '<START>' , '<END>'] #special tokens\n",
        "  words = words + [w for w in word_frequencies.keys() if word_frequencies[w] > min_word_freq]\n",
        "  vocabulary = {key : value for value,key in enumerate(words)}\n",
        "\n",
        "  #name for all prepared HDF5 files to save \n",
        "  nameHDF5 = f\"flicker8k_{captions_per_image}_captions_per_image_{min_word_freq}_min_freq_word\"\n",
        "\n",
        "  #saving vocabulary to a json file\n",
        "  with open(os.path.join(output_folder , f\"Vocabulary_{nameHDF5}.json\") , 'w') as vocab_f:\n",
        "    json.dump(vocabulary , vocab_f)\n",
        "\n",
        "  seed(55)\n",
        "  #sampling of a caption for each image and saving images to a HDF5 file ,\n",
        "  # and captions , caption lengths to json files \n",
        "   \n",
        "  \n",
        "  for image_paths , image_captions ,split in [(training_images_paths ,training_images_captions ,\"TRAIN\"),\n",
        "                                            (val_images_paths , val_images_captions , \"VAL\"),\n",
        "                                            (test_images_paths , test_images_captions , \"TEST\")]:\n",
        "    if os.path.exists(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\")) == False:\n",
        "      with h5py.File(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\") , 'a') as hfile:\n",
        "        hfile.attrs[\"Captions_per_image\"] = captions_per_image \n",
        "        # create a dataset inside hdf5 for images \n",
        "        images = hfile.create_dataset('images' , (len(image_paths) , 3 ,256,256) , dtype = 'uint') \n",
        "        #read image and save it in h5py\n",
        "        im = imread(image_paths[index])\n",
        "        if len(im.shape) == 2 :\n",
        "            im = im[: , : , np.newaxis]\n",
        "            im = np.concatenate([im , im ,im] ,axis = -1)\n",
        "        im = np.array(Image.fromarray(im).resize(size = (256,256)))\n",
        "        im = im.transpose(2,0,1)\n",
        "          \n",
        "          \n",
        "        assert im.shape == (3,256,256) \n",
        "        assert np.max(im) <= 255\n",
        "\n",
        "        images[index] = im #saving to hdf5 \n",
        "\n",
        "    encoded_indexed_captions ,caption_lengths = [] , []\n",
        "    for index , path in enumerate(tqdm(image_paths)):\n",
        "          \n",
        "          #sample captions \n",
        "          if len(image_captions[index]) < captions_per_image :\n",
        "            # re add caption from available ones to make no of captions equal to captions_per_image\n",
        "            captions = image_captions[index] + [choice(image_captions[index]) for _ in range(captions_per_image-image_captions)]\n",
        "          else :\n",
        "            captions = sample(image_captions[index] , k=captions_per_image)\n",
        "          \n",
        "\n",
        "          assert len(captions) == captions_per_image\n",
        "          \n",
        "          #encoding captions to index of their tokens\n",
        "          for cap_no , caption in enumerate(captions):\n",
        "\n",
        "              encoded_caption = [vocabulary[\"<START>\"]] + [vocabulary.get(word ,vocabulary[\"<UNK>\"]) for word in caption] \\\n",
        "               + [vocabulary[\"<END>\"]] + [vocabulary[\"<PAD>\"]*(max_len-len(caption))]\n",
        "              caption_len = len(caption) + 2\n",
        "              encoded_indexed_captions.append(caption)\n",
        "              caption_lengths.append(caption_len)\n",
        "\n",
        "    assert len(encoded_indexed_captions) == len(caption_lengths) == len(image_paths) * captions_per_image\n",
        "    print(f\"\\nEncoded captions for {split} : {len(encoded_indexed_captions)}\")\n",
        "    with open(os.path.join(output_folder , f\"_Encoded_captions_{split}_{nameHDF5}.json\") , 'w') as f:\n",
        "        json.dump(encoded_indexed_captions ,f )\n",
        "\n",
        "    with open(os.path.join(output_folder , f\"_captions_lengths_{split}_{nameHDF5}'.json\") , 'w') as f:\n",
        "        json.dump(caption_lengths , f)\n",
        "            \n",
        "        "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHIQf-lNbkV"
      },
      "source": [
        "json_path = \"./drive/MyDrive/flicker8k/dataset_flickr8k.json\"\n",
        "img_folder = \"./drive/MyDrive/flicker8k/Images\"\n",
        "out_folder = \"./drive/MyDrive/flicker8k/processed_input_files\"\n",
        "\n",
        "#create_input_files(json_path,img_folder,5,5,out_folder)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcSUCA51KbUy"
      },
      "source": [
        "class DataSet(Dataset):\n",
        "  \"\"\" Dateset class to be used in Dataloader to create batches \"\"\"\n",
        "  \n",
        "  def __init__(self , dir ,base_name , split , transform= None ):\n",
        "    \"\"\"\n",
        "      parameter : \n",
        "\n",
        "        dir : folder with data files\n",
        "        base_name : common base name used to save hdf5 files\n",
        "        split : in { \"TRAIN\"  , \"VAL\" , \"TEST\" }\n",
        "        transform : image transformations pipeline\n",
        "    \"\"\"\n",
        "    self.dir = dir \n",
        "    self.split = split \n",
        "    assert self.split in {\"TRAIN\" , \"VAL\" , \"TEST\"} , \"split is not specidied correctly\"\n",
        "    self.transform = transform \n",
        "    #reading the data stored in hdf5 file\n",
        "    self.data = h5py.File(os.path.join(dir , f\"{self.split}_IMAGES_{base_name}.hdf5\") , 'r')\n",
        "    self.images =  self.data[\"images\"]\n",
        "    self.caps_per_im = self.data.attrs[\"captions_per_image\"]\n",
        "\n",
        "    #loading encoded captions completely into memory\n",
        "    with open(os.path.join(dir , f\"_Encoded_captions_{self.split}_{base_name}.json\") , 'r') as cap_f:\n",
        "        self.captions = json.load(cap_f)\n",
        "    #loading encoded captions length completely into memory\n",
        "\n",
        "    with open(os.path.join(dir  , f\"_captions_lengths_{self.split}_{base_name}.json\")) as cap_f :\n",
        "        self.caps_len = json.load(cap_f)\n",
        "    \n",
        "    self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self , index) :\n",
        "        #Nth caption corespond to (N//captions_per_image)th image and iamges will be fed according to captions\n",
        "        image = torch.FloatTensor(self.images[index // self.caps_per_im / 255.0]) #normlizing by 255\n",
        "        if self.transform is not None :\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_of_image = torch.FloatTensor(self.captions[index])\n",
        "        caption_length = torch.FloatTensor(self.caps_len[index])\n",
        "        if self.split == \"TRAIN\"    :\n",
        "            return image ,caption_of_image , caption_length \n",
        "        else:\n",
        "            # for validation and testing we need all captions to find BLEU-4 score\n",
        "\n",
        "            all_captions = torch.FloatTensor(self.captions[\n",
        "                (index//self.caps_per_im) * self.caps_per_len : (index//self.caps_per_im)*self.caps_per_len + self.caps_per_im\n",
        "                ])\n",
        "            return image , caption_of_image , caption_length ,all_captions\n",
        "        "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LoG4z7KbJl"
      },
      "source": [
        "device  = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRlg8SFoKa7U"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder that uses a pretrained model to extract features embedded vectors from images \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self , image_embedding_size = 14):\n",
        "        super(Encoder , self  ).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        wide_resnet = torchvision.models.wide_resnet50_2(pretrained=True , progress= True)\n",
        "        Blocks_to_keep = list(wide_resnet.children())[:-2] # we drop the last two avgpooling and linear layers\n",
        "        self.features_extractor = nn.Sequential(*blocks_to_keep)\n",
        "\n",
        "        #to allow inputs of various resolution we use adaptive avg pooling which adjusts strides ,kernel size\n",
        "        #and outputs an image of given size\n",
        "\n",
        "        self.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d((image_embedding_size , image_embedding_size))\n",
        "\n",
        "        for p in self.features_extractor.parameters():\n",
        "             p.requires_grad = False\n",
        "\n",
        "    def forward(self , images):\n",
        "        \"\"\"\n",
        "        applies forward pass\n",
        "\n",
        "        parameters : images -> a tensor of shape [ batch_size , channels , image_input_size , image_input_size ]\n",
        "\n",
        "        \"\"\"\n",
        "        features = self.features_extractor(images) # results in shape [ batch_size , 2048, image_input_size/32 , image_input_size/32  ]\n",
        "        features = self.AdaptiveAvgPool2d(feature)  # results in shape [batch_size , 2048 ,image_embedding_size , image_embedding_size]\n",
        "        features = features.permute(0,2,3,1) #results in shape [batch_size , image_embedding_size , image_embedding_size , 3]\n",
        "        features = features.view(features.shape[0] , -1 , features.shape[-1]) # results in [batch_size , 14*14 , 2048]\n",
        "        return features\n",
        "\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJMkf6YP-w5"
      },
      "source": [
        "class AttentionNn(nn.Module):\n",
        "    \"\"\" A Nn to learn the attention mapping \"\"\"\n",
        "    def __init__(self ,encoder_output_dim , decoder_output_dim , attention_hidden_dim  ):\n",
        "        \"\"\"\n",
        "        parameters:\n",
        "            encoder_output_dim : dim of encoded image features embedding\n",
        "            decoder_output_dim : dim of decoder output word embedding\n",
        "            attention_hidden_dim : dim of attention neural network's hidden layer\n",
        "        \"\"\"\n",
        "        super(AttentionNn , self).__init__()\n",
        "        # transformation to same space\n",
        "        self.transformed_enc = nn.Linear(encoder_output_dim , attention_hidden_dim)\n",
        "        self.transformed_dec = nn.Linear(decoder_output_dim , attention_hidden_dim)\n",
        "\n",
        "        self.attention_map = nn.Sequential([\n",
        "            nn.Linear(attention_hidden_dim , attention_hidden_dim ) ,nn.LeakyReLU(0.2) ,\n",
        "            nn.Linear(attention_hidden_dim , 1) , nn.LeakyReLU(0.2) ,nn.Softmax(dim = 1)                                 \n",
        "        ])\n",
        "\n",
        "        def forward(self , image_embedding , word_embedding):\n",
        "            \"\"\"\n",
        "            Performs the attention mapping\n",
        "            Parameters :\n",
        "                image_embedding : output of feature extractor of shape [batch_size , num_pixels ,encoder_output_dim]\n",
        "                word_embedding : previous word output from decoder [batch_size , decoder_output_dim]\n",
        "            \"\"\"\n",
        "            enc_transformed = self.transformed_enc(image_embedding) #results in [batch_size , num_pixels , attention_hidden_dim]\n",
        "            dec_transformed = self.transformed_dec(word_embedding) #results in [batch_size  , attention_hidden_dim]\n",
        "            alpha_distribution = self.attention_map(nn.Tanh(enc_transformed + dec_transformed.unsqueeze(1))) #results in [batch_size ,num_pixels ,1]\n",
        "\n",
        "\n",
        "            # applying attention on  pixels and summing to get the  weighted-pixel embedding\n",
        "            attended_feature_embedding = torch.sum(image_embedding * alpha_distribution , dim = 1) # results in [batch_size , encoder_output_dim]\n",
        "            return attended_feature_embedding , alpha_distribution\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUZuSdn11HXJ"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder that decodes attended feature embeddings to output a textual description of features in image\n",
        "    \"\"\"\n",
        "    def __init__(self , attention_h_dim , word_embedding_size ,decoder_hidden_dim,\n",
        "                 vocab_size ,encoder_output_dim =2048, dropout = 0.2  ) : \n",
        "        \"\"\"\n",
        "        Parameters : \n",
        "            attention_h_dim : dim to use in attention Nn hidden layers\n",
        "            word_embedding_size : dim for word embeddings\n",
        "            decoder_hidden_dim : dim to use in LSTM hidden layers\n",
        "            vocab_size : size of vocabulary \n",
        "            encoder_output_dim : dimension of encoder's output vecto\n",
        "            dropout : drop probability for Dropout layers\n",
        "        \"\"\"\n",
        "        super(Decoder , self).__init__()\n",
        "        self.attention_h_dim = attention_h_dim\n",
        "        self.word_embedding_size = word_embedding_size\n",
        "        self.decoder_hidden_dim = decoder_hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.encoder_output_dim = encoder_output_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size , word_embedding_size)\n",
        "        self.attention = AttentionNn(encoder_output_dim , decoder_output_dim ,attention_h_dim)\n",
        "        self.dropout = nn.Dropout(p = self.dropout)\n",
        "        self.lstm = nn.LSTMCell(encoder_output_dim + word_embedding_size, decoder_hidden_dim)\n",
        "        self.lstm_h0 = nn.Linear(encoder_output_dim , decoder_hidden_dim)\n",
        "        self.lstm_c0 = nn.Linear(encoder_output_dim , decoder_hidden_dim)\n",
        "\n",
        "        #authors of \"Show attend and tell ....\"  recommend passing attention weighted encoding through a Sigmoid act gate\n",
        "        #that is a linar transform of ddecoder previous hidden stae\n",
        "        # this helps attention mechanism to put more emphasis on objects\n",
        "        self.gate_transform = nn.Linear(decoder_output_dim , encoder_output_dim )\n",
        "        self.gate = nn.Sigmoid()\n",
        "\n",
        "        # final layer to find score over vocabulary\n",
        "        self.final = nn.Linear(decoder_output_dim , vocab_size)\n",
        "\n",
        "        def initialize_hidden_states(self , encoder_output):\n",
        "            \"\"\"\n",
        "            this method initializes hidden states for lstm cell based on encoded images \n",
        "            \"\"\"\n",
        "            mean_encooding = encoder_output.mean(dim = 1)\n",
        "            h0 = self.lstm_h0(mean_encooding)\n",
        "            c0 = self.lstm_c0(mean_encoding)\n",
        "\n",
        "            return h0 , c0 \n",
        "\n",
        "        def use_pretrained_embeddings(self , embedding_weights):\n",
        "            self.word_embeddings.weight = nn.Parameter(embedding_weights)\n",
        "        def finetune_embeddings(self , fine_tune=True):\n",
        "            for p in self.word_embeddings.parameter():\n",
        "                p.requires_grad = fine_tune\n",
        "\n",
        "\n",
        "        def forward(self , image_embeddings , captions ,  caption_length):\n",
        "            \"\"\"\n",
        "            performs forward pass of decoder \n",
        "\n",
        "            Parameters:\n",
        "                image_embeddings : tensor of images with size [ batch_size , num_pixels ,encoder_output_dim]\n",
        "                captions : encoded captions of images with size [batch_Size , max_caption_length]\n",
        "                caption_lengths : caption lengths of size [batch size , 1]\n",
        "            \"\"\"\n",
        "            encoder_dim = image_embeddings.shape[-1]\n",
        "            batch_size = captions.shape[0]\n",
        "            vocab_size = self.vocab_size\n",
        "\n",
        "            #sorting images and captions in descending order of caption lengths\n",
        "            sorted_caption_lengths, sorted_indices = (captions.squeeze(1)).sort(dim = 0 ,descending = True)\n",
        "            image_embeddings = image_embeddings[sorted_indices]\n",
        "            captions = captions[sorted_indices]\n",
        "            # indexed tokens -> vectors\n",
        "            word_embeddings = self.word_embeddings(captions)\n",
        "             \n",
        "            h0 , c0 = initialize_hidden_states(image_embeddings) \n",
        "            # we dont need to go through <END> while decodiing\n",
        "            sequence_length = len(captions[0]) -1\n",
        "            decode_lengths = (caption_lengths -1).tolist()\n",
        "            # tensors to hold predictions  and alphas\n",
        "\n",
        "            predictions = torch.zeros(batch_size , sequence_length , vocab_size).to_device(device)\n",
        "            alphas = torch.zeros(batch_size , sequence_length , num_pixels).to_device(device)\n",
        "            \n",
        "            #We decode a word using previous word from decoder and image feature attended embedding \n",
        "            for time_step in range(sequence_length):\n",
        "                effective_batch_size = sum([  length  > time_step for length in decode_lengths])\n",
        "                attention_weighted_encoding , alpha_dist = self.attention(image_embeddings[:effective_batch_size ] ,\n",
        "                                                                          h0[:effective_batch_size])\n",
        "                gate = self.gate(self.gate_transform(h0[:effective_batch_size]))\n",
        "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "                h0,c0 = self.lstm(\n",
        "                    torch.cat([word_embeddings[:batch_size ,time_step ,: ] , attention_weighted_encoding] , dim = 1)\n",
        "                    (h0[:effective_batch_size] , c0[:effective_batch_size])\n",
        "                )\n",
        "                preds = self.final(self.dropout(h0))\n",
        "                predictions[:effective_batch_size] = preds\n",
        "                alphas[:effective_batch_size] = alpha_dist\n",
        "            return predictions ,encoded_captions . decode_lengths , alphas ,sorted_indices\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7W-deLl1lSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94318197-86ff-4de7-8ece-1ae2381de732"
      },
      "source": [
        "decode_lengths = [ 9,8,6,4,3,2]\n",
        "for t in range(9):\n",
        "    batch_size_t = sum([l > t for l in decode_lengths])\n",
        "    print(batch_size_t)\n",
        "                    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4XFhHSdDsOA",
        "outputId": "15ac6baf-2008-49be-b106-c90b82ef213e"
      },
      "source": [
        "print([l > 8 for l in decode_lengths])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[True, False, False, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPNTdxPB19fk"
      },
      "source": [
        "caption_lengths = torch.randn(20,1)\n",
        "encoder_out = torch.randn(20,4,10)\n",
        "encoded_captions = torch.randn(20,5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trYzCokM2WPV",
        "outputId": "9ff77175-2987-4d5e-8e08-dfc160df0993"
      },
      "source": [
        "caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "print(sort_ind.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X2pwgckESW4"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTvwTZ1cynlH"
      },
      "source": [
        "encoder_out = encoder_out[sort_ind]\n",
        "encoded_captions = encoded_captions[sort_ind]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29HOTGRn4FUS",
        "outputId": "6b839ff6-f300-484d-d3f0-6f9974ef5d71"
      },
      "source": [
        "encoder_out[:5].shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8VRjIUbaFX",
        "outputId": "67d806f7-8921-48c3-d3b7-d2e646806c06"
      },
      "source": [
        "ca = torch.Tensor([10,9,6,7,8,6,7,6,])\n",
        "print((ca - 1).tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.0, 8.0, 5.0, 6.0, 7.0, 5.0, 6.0, 5.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehKVQ5EweLjA",
        "outputId": "ab130faa-670a-4cdb-839f-a1607a79c588"
      },
      "source": [
        "encoder_out[:12].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 4, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXT5q5iplRF_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}