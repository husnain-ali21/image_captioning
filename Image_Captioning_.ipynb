{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fq6kG-TXCrgU8ONUXxrmhqVrjZuiDRhV",
      "authorship_tag": "ABX9TyMsrEJ0NPZyjVo1cIXgx5P4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojo1sDNILR2B"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import h5py \n",
        "import json \n",
        "import torch\n",
        "from imageio import  imread #or import matplotlib.image as mpimg and mpimg.imread\n",
        "from PIL import Image #resized_img = Image.fromarray(orj_img).resize(size=(new_h, new_w))\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed ,choice ,sample\n",
        "from torch.utils.data  import Dataset,DataLoader\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXyF3niTL4-O"
      },
      "source": [
        "import torchvision\n",
        "from torch import nn"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C15PcLvlnkfX"
      },
      "source": [
        "def create_input_files( karpathy_json_path ,image_folder, captions_per_image \n",
        "                       ,min_word_freq , output_folder ,max_len = 50): \n",
        "  \"\"\"\n",
        "  Funtion that prepares data for training,validation and testing and\n",
        "  saves in HDF5 and json format as it is an efficient way of loading data using pyTorch.\n",
        "\n",
        "  Parameters : \n",
        "  -karpathy_json_path : path for the splits and captions\n",
        "  -image_folder : folder with downloaded images \n",
        "  -captions_per_image : no of captions to sample per image\n",
        "  -min_word_freq : a threshold for words to be declared as <UNK>s\n",
        "  -output_folder : folder to save prepared h5py files \n",
        "  -max_len : maximum sequence length \n",
        "  \"\"\"\n",
        "\n",
        "  with open(karpathy_json_path , 'r') as f :\n",
        "    data = json.load(f)\n",
        "  \n",
        "  training_images_paths ,training_images_captions = [] ,[]\n",
        "  val_images_paths , val_images_captions = [],[]\n",
        "  test_images_paths , test_images_captions = [],[]\n",
        "  word_frequencies = Counter()\n",
        "\n",
        "  for image in data[\"images\"]:\n",
        "    captions = [] #a list that stores all appropriate captions of an image \n",
        "    for each_caption in image[\"sentences\"]:\n",
        "      #update word_frequencies\n",
        "      word_frequencies.update(each_caption[\"tokens\"])\n",
        "      if len(each_caption[\"tokens\"]) <= max_len:#discard too long captions\n",
        "        captions.append(each_caption[\"tokens\"])\n",
        "\n",
        "    if len(captions) ==  0 :\n",
        "        continue #we skip the image as we don't need to do anything further\n",
        "      \n",
        "    path_to_an_image = os.path.join(image_folder ,image[\"filename\"])\n",
        "\n",
        "    if image['split'] in {'train' , 'restval'} :\n",
        "      training_images_paths.append(path_to_an_image)\n",
        "      training_images_captions.append(captions)\n",
        "    elif image['split'] == 'val' :\n",
        "      val_images_paths.append(path_to_an_image)\n",
        "      val_images_captions.append(captions)\n",
        "    else :\n",
        "      test_images_paths.append(path_to_an_image)\n",
        "      test_images_captions.append(captions)\n",
        "    \n",
        "  assert len(training_images_paths) == len(training_images_captions) \n",
        "  assert len(val_images_paths) == len(val_images_captions)\n",
        "  assert len(test_images_paths) == len(test_images_captions)\n",
        "\n",
        "  print(f'training dataset size :{len(training_images_paths)}')\n",
        "  print(f'validation dataset size :{len(val_images_paths)}')\n",
        "  print(f'test dataset size :{len(test_images_paths)}')\n",
        "\n",
        "  #Vocabulary\n",
        "  words = ['<PAD>' ,'<UNK>' , '<START>' , '<END>'] #special tokens\n",
        "  words = words + [w for w in word_frequencies.keys() if word_frequencies[w] > min_word_freq]\n",
        "  vocabulary = {key : value for value,key in enumerate(words)}\n",
        "\n",
        "  #name for all prepared HDF5 files to save \n",
        "  nameHDF5 = f\"flicker8k_{captions_per_image}_captions_per_image_{min_word_freq}_min_freq_word\"\n",
        "\n",
        "  #saving vocabulary to a json file\n",
        "  with open(os.path.join(output_folder , f\"Vocabulary_{nameHDF5}.json\") , 'w') as vocab_f:\n",
        "    json.dump(vocabulary , vocab_f)\n",
        "\n",
        "  seed(55)\n",
        "  #sampling of a caption for each image and saving images to a HDF5 file ,\n",
        "  # and captions , caption lengths to json files \n",
        "   \n",
        "  \n",
        "  for image_paths , image_captions ,split in [(training_images_paths ,training_images_captions ,\"TRAIN\"),\n",
        "                                            (val_images_paths , val_images_captions , \"VAL\"),\n",
        "                                            (test_images_paths , test_images_captions , \"TEST\")]:\n",
        "    if os.path.exists(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\")) == False:\n",
        "      with h5py.File(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\") , 'a') as hfile:\n",
        "        hfile.attrs[\"Captions_per_image\"] = captions_per_image \n",
        "        # create a dataset inside hdf5 for images \n",
        "        images = hfile.create_dataset('images' , (len(image_paths) , 3 ,256,256) , dtype = 'uint') \n",
        "        #read image and save it in h5py\n",
        "        im = imread(image_paths[index])\n",
        "        if len(im.shape) == 2 :\n",
        "            im = im[: , : , np.newaxis]\n",
        "            im = np.concatenate([im , im ,im] ,axis = -1)\n",
        "        im = np.array(Image.fromarray(im).resize(size = (256,256)))\n",
        "        im = im.transpose(2,0,1)\n",
        "          \n",
        "          \n",
        "        assert im.shape == (3,256,256) \n",
        "        assert np.max(im) <= 255\n",
        "\n",
        "        images[index] = im #saving to hdf5 \n",
        "\n",
        "    encoded_indexed_captions ,caption_lengths = [] , []\n",
        "    for index , path in enumerate(tqdm(image_paths)):\n",
        "          \n",
        "          #sample captions \n",
        "          if len(image_captions[index]) < captions_per_image :\n",
        "            # re add caption from available ones to make no of captions equal to captions_per_image\n",
        "            captions = image_captions[index] + [choice(image_captions[index]) for _ in range(captions_per_image-image_captions)]\n",
        "          else :\n",
        "            captions = sample(image_captions[index] , k=captions_per_image)\n",
        "          \n",
        "\n",
        "          assert len(captions) == captions_per_image\n",
        "          \n",
        "          #encoding captions to index of their tokens\n",
        "          for cap_no , caption in enumerate(captions):\n",
        "\n",
        "              encoded_caption = [vocabulary[\"<START>\"]] + [vocabulary.get(word ,vocabulary[\"<UNK>\"]) for word in caption] \\\n",
        "               + [vocabulary[\"<END>\"]] + [vocabulary[\"<PAD>\"]*(max_len-len(caption))]\n",
        "              caption_len = len(caption) + 2\n",
        "              encoded_indexed_captions.append(caption)\n",
        "              caption_lengths.append(caption_len)\n",
        "\n",
        "    assert len(encoded_indexed_captions) == len(caption_lengths) == len(image_paths) * captions_per_image\n",
        "    print(f\"\\nEncoded captions for {split} : {len(encoded_indexed_captions)}\")\n",
        "    with open(os.path.join(output_folder , f\"_Encoded_captions_{split}_{nameHDF5}.json\") , 'w') as f:\n",
        "        json.dump(encoded_indexed_captions ,f )\n",
        "\n",
        "    with open(os.path.join(output_folder , f\"_captions_lengths_{split}_{nameHDF5}'.json\") , 'w') as f:\n",
        "        json.dump(caption_lengths , f)\n",
        "            \n",
        "        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHIQf-lNbkV"
      },
      "source": [
        "json_path = \"./drive/MyDrive/flicker8k/dataset_flickr8k.json\"\n",
        "img_folder = \"./drive/MyDrive/flicker8k/Images\"\n",
        "out_folder = \"./drive/MyDrive/flicker8k/processed_input_files\"\n",
        "\n",
        "#create_input_files(json_path,img_folder,5,5,out_folder)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcSUCA51KbUy"
      },
      "source": [
        "class DataSet(Dataset):\n",
        "  \"\"\" Dateset class to be used in Dataloader to create batches \"\"\"\n",
        "  \n",
        "  def __init__(self , dir ,base_name , split , transform= None ):\n",
        "    \"\"\"\n",
        "      parameter : \n",
        "\n",
        "        dir : folder with data files\n",
        "        base_name : common base name used to save hdf5 files\n",
        "        split : in { \"TRAIN\"  , \"VAL\" , \"TEST\" }\n",
        "        transform : image transformations pipeline\n",
        "    \"\"\"\n",
        "    self.dir = dir \n",
        "    self.split = split \n",
        "    assert self.split in {\"TRAIN\" , \"VAL\" , \"TEST\"} , \"split is not specidied correctly\"\n",
        "    self.transform = transform \n",
        "    #reading the data stored in hdf5 file\n",
        "    self.data = h5py.File(os.path.join(dir , f\"{self.split}_IMAGES_{base_name}.hdf5\") , 'r')\n",
        "    self.images =  self.data[\"images\"]\n",
        "    self.caps_per_im = self.data.attrs[\"captions_per_image\"]\n",
        "\n",
        "    #loading encoded captions completely into memory\n",
        "    with open(os.path.join(dir , f\"_Encoded_captions_{self.split}_{base_name}.json\") , 'r') as cap_f:\n",
        "        self.captions = json.load(cap_f)\n",
        "    #loading encoded captions length completely into memory\n",
        "\n",
        "    with open(os.path.join(dir  , f\"_captions_lengths_{self.split}_{base_name}.json\")) as cap_f :\n",
        "        self.caps_len = json.load(cap_f)\n",
        "    \n",
        "    self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self , index) :\n",
        "        #Nth caption corespond to (N//captions_per_image)th image and iamges will be fed according to captions\n",
        "        image = torch.FloatTensor(self.images[index // self.caps_per_im / 255.0]) #normlizing by 255\n",
        "        if self.transform is not None :\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_of_image = torch.FloatTensor(self.captions[index])\n",
        "        caption_length = torch.FloatTensor(self.caps_len[index])\n",
        "        if self.split == \"TRAIN\"    :\n",
        "            return image ,caption_of_image , caption_length \n",
        "        else:\n",
        "            # for validation and testing we need all captions to find BLEU-4 score\n",
        "\n",
        "            all_captions = torch.FloatTensor(self.captions[\n",
        "                (index//self.caps_per_im) * self.caps_per_len : (index//self.caps_per_im)*self.caps_per_len + self.caps_per_im\n",
        "                ])\n",
        "            return image , caption_of_image , caption_length ,all_captions\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LoG4z7KbJl"
      },
      "source": [
        "device  = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRlg8SFoKa7U"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder that uses a pretrained model to extract features embedded vectors from images \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self , image_embedding_size = 14)\n",
        "    super(Encoder , self  ).__init__()\n",
        "\n",
        "    self.embedding_size = embedding_size\n",
        "    wide_resnet = torchvision.models.wide_resnet50_2(pretrained=True , progress= True)\n",
        "    Blocks_to_keep = list(wide_resnet.children())[:-2] # we drop the last two avgpooling and linear layers\n",
        "    self.features_extractor = nn.Sequential(*blocks_to_keep)\n",
        "\n",
        "    #to allow inputs of various resolution we use adaptive avg pooling which adjusts strides ,kernel size\n",
        "    #and outputs an image of given size\n",
        "\n",
        "    self.AdaptiveAvgPool2d = nn.AdaptiveAvgPool2d((image_embedding_size , image_embedding_size))\n",
        "\n",
        "    for p in self.features_extractor.parameters():\n",
        "        p.requires_grad() = False\n",
        "\n",
        "    def forward(self , images):\n",
        "        \"\"\"\n",
        "        applies forward pass\n",
        "\n",
        "        parameters : images -> a tensor of shape [ batch_size , channels , image_input_size , image_input_size ]\n",
        "\n",
        "        \"\"\"\n",
        "        features = self.features_extractor(images) # results in shape [ batch_size , 2048, image_input_size/32 , image_input_size/32  ]\n",
        "        features = self.AdaptiveAvgPool2d(feature)  # results in shape [batch_size , 2048 ,image_embedding_size , image_embedding_size]\n",
        "        features = features.permute(0,2,3,1) #results in shape [batch_size , image_embedding_size , image_embedding_size , 3]\n",
        "        features = features.view(features.shape[0] , -1 , features.shape[-1]) # results in [batch_size , 14*14 , 2048]\n",
        "        return features\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJMkf6YP-w5"
      },
      "source": [
        "class AttentionNn(nn.Module):\n",
        "    \"\"\" A Nn to learn the attention mapping \"\"\"\n",
        "    def __init__(self ,encoder_output_dim , decoder_output_dim , attention_hidden_dim  ):\n",
        "        \"\"\"\n",
        "        parameters:\n",
        "            encoder_output_dim : dim of encoded image features embedding\n",
        "            decoder_output_dim : dim of decoder output word embedding\n",
        "            attention_hidden_dim : dim of attention neural network's hidden layer\n",
        "        \"\"\"\n",
        "        super(AttentionNn , self).__init__()\n",
        "        # transformation to same space\n",
        "        self.transformed_enc = nn.Linear(encoder_output_dim , attention_hidden_dim)\n",
        "        self.transformed_dec = nn.Linear(decoder_output_dim , attention_hidden_dim)\n",
        "\n",
        "        self.attention_map = nn.Sequential([\n",
        "            nn.Linear(attention_hidden_dim , attention_hidden_dim ) ,nn.LeakyReLU(0.2) ,\n",
        "            nn.Linear(attention_hidden_dim , 1) , nn.LeakyReLU(0.2) ,nn.Softmax(dim = 1)                                 \n",
        "        ])\n",
        "\n",
        "        def forward(self , image_embedding , word_embedding):\n",
        "            \"\"\"\n",
        "            Performs the attention mapping\n",
        "            Parameters :\n",
        "                image_embedding : output of feature extractor of shape [batch_size , num_pixels ,encoder_output_dim]\n",
        "                word_embedding : previous word output from decoder [batch_size , decoder_output_dim]\n",
        "            \"\"\"\n",
        "            enc_transformed = self.transformed_enc(image_embedding) #results in [batch_size , num_pixels , attention_hidden_dim]\n",
        "            dec_transformed = self.transformed_dec(word_embedding) #results in [batch_size  , attention_hidden_dim]\n",
        "            alpha_distribution = self.attention_map(nn.Tanh(enc_transformed + dec_transformed.unsqueeze(1))) #results in [batch_size ,num_pixels ,1]\n",
        "\n",
        "\n",
        "            # applying attention on  pixels and summing to get the  weighted-pixel embedding\n",
        "            attended_feature_embedding = torch.sum(image_embedding * alpha_distribution , dim = 1) # results in [batch_size , encoder_output_dim]\n",
        "            return attended_feature_embedding , alpha_distribution\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUZuSdn11HXJ"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7W-deLl1lSy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPNTdxPB19fk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trYzCokM2WPV"
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTvwTZ1cynlH",
        "outputId": "2e946248-a870-44fd-b344-ff1320d06e5a"
      },
      "source": [
        ""
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29HOTGRn4FUS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}