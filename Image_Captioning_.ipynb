{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fq6kG-TXCrgU8ONUXxrmhqVrjZuiDRhV",
      "authorship_tag": "ABX9TyPsSYa9d+7bYer66uSdMQM2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/husnain-ali21/image_captioning/blob/main/Image_Captioning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojo1sDNILR2B"
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import h5py \n",
        "import json \n",
        "import torch\n",
        "from imageio import  imread #or import matplotlib.image as mpimg and mpimg.imread\n",
        "from PIL import Image #resized_img = Image.fromarray(orj_img).resize(size=(new_h, new_w))\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed ,choice ,sample\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXyF3niTL4-O"
      },
      "source": [
        "from torch.utils.data  import Dataset,DataLoader\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C15PcLvlnkfX"
      },
      "source": [
        "def create_input_files( karpathy_json_path ,image_folder, captions_per_image \n",
        "                       ,min_word_freq , output_folder ,max_len = 50): \n",
        "  \"\"\"\n",
        "  Funtion that prepares data for training,validation and testing and\n",
        "  saves in HDF5 and json format as it is an efficient way of loading data using pyTorch.\n",
        "\n",
        "  Parameters : \n",
        "  -karpathy_json_path : path for the splits and captions\n",
        "  -image_folder : folder with downloaded images \n",
        "  -captions_per_image : no of captions to sample per image\n",
        "  -min_word_freq : a threshold for words to be declared as <UNK>s\n",
        "  -output_folder : folder to save prepared h5py files \n",
        "  -max_len : maximum sequence length \n",
        "  \"\"\"\n",
        "\n",
        "  with open(karpathy_json_path , 'r') as f :\n",
        "    data = json.load(f)\n",
        "  \n",
        "  training_images_paths ,training_images_captions = [] ,[]\n",
        "  val_images_paths , val_images_captions = [],[]\n",
        "  test_images_paths , test_images_captions = [],[]\n",
        "  word_frequencies = Counter()\n",
        "\n",
        "  for image in data[\"images\"]:\n",
        "    captions = [] #a list that stores all appropriate captions of an image \n",
        "    for each_caption in image[\"sentences\"]:\n",
        "      #update word_frequencies\n",
        "      word_frequencies.update(each_caption[\"tokens\"])\n",
        "      if len(each_caption[\"tokens\"]) <= max_len:#discard too long captions\n",
        "        captions.append(each_caption[\"tokens\"])\n",
        "\n",
        "    if len(captions) ==  0 :\n",
        "        continue #we skip the image as we don't need to do anything further\n",
        "      \n",
        "    path_to_an_image = os.path.join(image_folder ,image[\"filename\"])\n",
        "\n",
        "    if image['split'] in {'train' , 'restval'} :\n",
        "      training_images_paths.append(path_to_an_image)\n",
        "      training_images_captions.append(captions)\n",
        "    elif image['split'] == 'val' :\n",
        "      val_images_paths.append(path_to_an_image)\n",
        "      val_images_captions.append(captions)\n",
        "    else :\n",
        "      test_images_paths.append(path_to_an_image)\n",
        "      test_images_captions.append(captions)\n",
        "    \n",
        "  assert len(training_images_paths) == len(training_images_captions) \n",
        "  assert len(val_images_paths) == len(val_images_captions)\n",
        "  assert len(test_images_paths) == len(test_images_captions)\n",
        "\n",
        "  print(f'training dataset size :{len(training_images_paths)}')\n",
        "  print(f'validation dataset size :{len(val_images_paths)}')\n",
        "  print(f'test dataset size :{len(test_images_paths)}')\n",
        "\n",
        "  #Vocabulary\n",
        "  words = ['<PAD>' ,'<UNK>' , '<START>' , '<END>'] #special tokens\n",
        "  words = words + [w for w in word_frequencies.keys() if word_frequencies[w] > min_word_freq]\n",
        "  vocabulary = {key : value for value,key in enumerate(words)}\n",
        "\n",
        "  #name for all prepared HDF5 files to save \n",
        "  nameHDF5 = f\"flicker8k_{captions_per_image}_captions_per_image_{min_word_freq}_min_freq_word\"\n",
        "\n",
        "  #saving vocabulary to a json file\n",
        "  with open(os.path.join(output_folder , f\"Vocabulary_{nameHDF5}.json\") , 'w') as vocab_f:\n",
        "    json.dump(vocabulary , vocab_f)\n",
        "\n",
        "  seed(55)\n",
        "  #sampling of a caption for each image and saving images to a HDF5 file ,\n",
        "  # and captions , caption lengths to json files \n",
        "   \n",
        "  \n",
        "  for image_paths , image_captions ,split in [(training_images_paths ,training_images_captions ,\"TRAIN\"),\n",
        "                                            (val_images_paths , val_images_captions , \"VAL\"),\n",
        "                                            (test_images_paths , test_images_captions , \"TEST\")]:\n",
        "    if os.path.exists(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\")) == False:\n",
        "      with h5py.File(os.path.join(output_folder ,f\"_{split}_IMAGES_{nameHDF5}.hdf5\") , 'a') as hfile:\n",
        "        hfile.attrs[\"Captions_per_image\"] = captions_per_image \n",
        "        # create a dataset inside hdf5 for images \n",
        "        images = hfile.create_dataset('images' , (len(image_paths) , 3 ,256,256) , dtype = 'uint') \n",
        "        #read image and save it in h5py\n",
        "        im = imread(image_paths[index])\n",
        "        if len(im.shape) == 2 :\n",
        "            im = im[: , : , np.newaxis]\n",
        "            im = np.concatenate([im , im ,im] ,axis = -1)\n",
        "        im = np.array(Image.fromarray(im).resize(size = (256,256)))\n",
        "        im = im.transpose(2,0,1)\n",
        "          \n",
        "          \n",
        "        assert im.shape == (3,256,256) \n",
        "        assert np.max(im) <= 255\n",
        "\n",
        "        images[index] = im #saving to hdf5 \n",
        "\n",
        "    encoded_indexed_captions ,caption_lengths = [] , []\n",
        "    for index , path in enumerate(tqdm(image_paths)):\n",
        "          \n",
        "          #sample captions \n",
        "          if len(image_captions[index]) < captions_per_image :\n",
        "            # re add caption from available ones to make no of captions equal to captions_per_image\n",
        "            captions = image_captions[index] + [choice(image_captions[index]) for _ in range(captions_per_image-image_captions)]\n",
        "          else :\n",
        "            captions = sample(image_captions[index] , k=captions_per_image)\n",
        "          \n",
        "\n",
        "          assert len(captions) == captions_per_image\n",
        "          \n",
        "          #encoding captions to index of their tokens\n",
        "          for cap_no , caption in enumerate(captions):\n",
        "\n",
        "              encoded_caption = [vocabulary[\"<START>\"]] + [vocabulary.get(word ,vocabulary[\"<UNK>\"]) for word in caption] \\\n",
        "               + [vocabulary[\"<END>\"]] + [vocabulary[\"<PAD>\"]*(max_len-len(caption))]\n",
        "              caption_len = len(caption) + 2\n",
        "              encoded_indexed_captions.append(caption)\n",
        "              caption_lengths.append(caption_len)\n",
        "\n",
        "    assert len(encoded_indexed_captions) == len(caption_lengths) == len(image_paths) * captions_per_image\n",
        "    print(f\"\\nEncoded captions for {split} : {len(encoded_indexed_captions)}\")\n",
        "    with open(os.path.join(output_folder , f\"_Encoded_captions_{split}_{nameHDF5}.json\") , 'w') as f:\n",
        "        json.dump(encoded_indexed_captions ,f )\n",
        "\n",
        "    with open(os.path.join(output_folder , f\"_captions_lengths_{split}_{nameHDF5}'.json\") , 'w') as f:\n",
        "        json.dump(caption_lengths , f)\n",
        "            \n",
        "        "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSHIQf-lNbkV",
        "outputId": "7649dd39-01bf-4d31-dde3-b25fe8d4b17c"
      },
      "source": [
        "json_path = \"./drive/MyDrive/flicker8k/dataset_flickr8k.json\"\n",
        "img_folder = \"./drive/MyDrive/flicker8k/Images\"\n",
        "out_folder = \"./drive/MyDrive/flicker8k/processed_input_files\"\n",
        "\n",
        "#create_input_files(json_path,img_folder,5,5,out_folder)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 2283/6000 [00:00<00:00, 22769.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training dataset size :6000\n",
            "validation dataset size :1000\n",
            "test dataset size :1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:00<00:00, 24920.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoded captions for TRAIN : 30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 18455.16it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoded captions for VAL : 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1000/1000 [00:00<00:00, 29958.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoded captions for TEST : 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcSUCA51KbUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "b4eb609d-ceec-40a4-8fce-a60c258e6aad"
      },
      "source": [
        "class DataSet(Dataset):\n",
        "  \"\"\" Dateset class to be used in Dataloader to create batches \"\"\"\n",
        "  \n",
        "  def __init__(self , dir ,base_name , split , transform= None ):\n",
        "      \"\"\"\n",
        "      parameter : \n",
        "\n",
        "        dir : folder with data files\n",
        "        base_name : common base name used to save hdf5 files\n",
        "        split : in { \"TRAIN\"  , \"VAL\" , \"TEST\" }\n",
        "        transform : image transformations pipeline\n",
        "      \"\"\"\n",
        "    self.dir = dir \n",
        "    self.split = split \n",
        "    assert self.split in {\"TRAIN\" , \"VAL\" , \"TEST\"} , \"split is not specidied correctly\"\n",
        "    self.transform = transform \n",
        "    #reading the data stored in hdf5 file\n",
        "    self.data = h5py.File(os.path.join(dir , f\"{self.split}_IMAGES_{base_name}.hdf5\") , 'r')\n",
        "    self.images =  self.data[\"images\"]\n",
        "    self.caps_per_im = self.data.attrs[\"captions_per_image\"]\n",
        "\n",
        "    #loading encoded captions completely into memory\n",
        "    with open(os.path.join(dir , f\"_Encoded_captions_{self.split}_{base_name}.json\") , 'r') as cap_f:\n",
        "        self.captions = json.load(cap_f)\n",
        "    #loading encoded captions length completely into memory\n",
        "\n",
        "    with open(os.path.join(dir  , f\"_captions_lengths_{self.split}_{base_name}.json\")) as cap_f :\n",
        "        self.caps_len = json.load(cap_f)\n",
        "    \n",
        "    self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self , index) :\n",
        "        #Nth caption corespond to (N//captions_per_image)th image and iamges will be fed according to captions\n",
        "        image = torch.FloatTensor(self.images[index // self.caps_per_im / 255.0]) #normlizing by 255\n",
        "        if self.transform is not None :\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption_of_image = torch.FloatTensor(self.captions[index])\n",
        "        caption_length = torch.FloatTensor(self.caps_len[index])\n",
        "        if self.split == \"TRAIN\"    :\n",
        "            return image ,caption_of_image , caption_length \n",
        "        else:\n",
        "            # for validation and testing we need all captions to find BLEU-4 score\n",
        "\n",
        "            all_captions = torch.FloatTensor(self.captions[\n",
        "                (index//self.caps_per_im) * self.caps_per_len : (index//self.caps_per_im)*self.caps_per_len + self.caps_per_im\n",
        "                ])\n",
        "            return image , caption_of_image , caption_length ,all_captions\n",
        "        \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-bb3b969727ca>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def __init__(self ,):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LoG4z7KbJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688b15cd-b6fb-44bb-cb8b-5f86d0ebf222"
      },
      "source": [
        "26//5"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRlg8SFoKa7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca6c34c-cf10-4fa8-e384-c1117ea570d5"
      },
      "source": [
        "cap_lens = [1,2,3,4]\n",
        "print(torch.LongTensor([cap_lens[3]]))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJMkf6YP-w5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}